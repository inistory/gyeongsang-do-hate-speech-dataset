from transformers import AutoConfig  # ìƒë‹¨ì— ì¶”ê°€
import os  # ìƒë‹¨ì— ì¶”ê°€
from torch import nn
import numpy as np
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments,
    DataCollatorForTokenClassification
)
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# ëª¨ë¸ ì„¤ì •
model_name = "beomi/KcELECTRA-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
label_all_tokens = False

# í‰ê°€ í•¨ìˆ˜
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_labels = []
    true_preds = []
    for pred, label in zip(predictions, labels):
        for p_, l_ in zip(pred, label):
            if l_ != -100:  # padding ë¬´ì‹œ
                true_preds.append(p_)
                true_labels.append(l_)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_preds, average='binary')
    acc = accuracy_score(true_labels, true_preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def tokenize_and_align_labels(example):
    tokenized = tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)
    word_ids = tokenized.word_ids()
    labels = example["labels"]
    aligned_labels = []
    for idx in range(len(word_ids)):
        word_idx = word_ids[idx]
        if word_idx is None:
            aligned_labels.append(-100)
        elif label_all_tokens:
            aligned_labels.append(labels[word_idx])
        else:
            aligned_labels.append(labels[word_idx] if idx == 0 or word_ids[idx - 1] != word_idx else -100)
    tokenized["labels"] = aligned_labels
    return tokenized

# ë‹¨ê³„ë³„ í•™ìŠµ
prev_model_path = None
for stage in ["easy", "medium", "hard"]:
    print(f"\nğŸ“š Training on stage: {stage.upper()}")
    
    dataset = load_dataset("json", data_files={stage: f"detection_{stage}.json"})[stage]
    tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)

    model = AutoModelForTokenClassification.from_pretrained(
        prev_model_path or model_name,
        num_labels=2
    )
    model.classifier = nn.Linear(model.config.hidden_size, 2)  # ì¶œë ¥ ë ˆì´ì–´ë¥¼ 2ê°œ ë ˆì´ë¸”ë¡œ ì¬ì„¤ì •

    args = TrainingArguments(
        output_dir=f"./detection_model_{stage}",
        do_eval=True,                  # í‰ê°€ í™œì„±í™”
        do_train=True,                 # í•™ìŠµ í™œì„±í™”
        save_steps=500,                # ì €ì¥ ì£¼ê¸°
        eval_steps=500,                # í‰ê°€ ì£¼ê¸°
        save_total_limit=1,            # ì €ì¥í•  ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=2e-5,
        weight_decay=0.01,
        logging_dir=f"./logs_{stage}",
        report_to="none"              # ì¶”ê°€ ë³´ê³  ë¹„í™œì„±í™”
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_dataset,
        eval_dataset=tokenized_dataset.select(range(min(500, len(tokenized_dataset)))),
        tokenizer=tokenizer,
        data_collator=DataCollatorForTokenClassification(tokenizer),
        compute_metrics=compute_metrics,
    )

    trainer.train()
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
    if trainer.state.best_model_checkpoint:
        prev_model_path = trainer.state.best_model_checkpoint
    else:
        # ìµœìƒì˜ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ë§ˆì§€ë§‰ ëª¨ë¸ ì‚¬ìš©
        trainer.save_model()  # í˜„ì¬ ëª¨ë¸ ì €ì¥
        prev_model_path = args.output_dir
    
    print(f"âœ… Saved best model to: {prev_model_path}")
